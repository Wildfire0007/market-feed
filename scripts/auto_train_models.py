#!/usr/bin/env python3
"""Automate the Feature → Label → Train loop for configured assets.

This helper glues together the existing ``make_labels.py`` and
``train_models.py`` CLIs so a single command (or CI job) can refresh the
feature snapshots, derive labels and persist new GradientBoosting models.
"""
from __future__ import annotations

import argparse
import math
import os
import subprocess
import sys
from pathlib import Path
from typing import Iterable, List, Sequence

import pandas as pd

_PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(_PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(_PROJECT_ROOT))

from config.analysis_settings import ASSETS  # noqa: E402

DEFAULT_LABEL_METHOD = "tbm"
DEFAULT_HORIZON = 48
DEFAULT_THRESHOLD = 0.0
DEFAULT_PT = 0.01
DEFAULT_SL = 0.01
DEFAULT_MIN_SAMPLES = 25
DEFAULT_TRIGGER_COLUMN = "precision_trigger_fire"
DEFAULT_TBM_PT_FLOOR = 0.001
MAX_TBM_PT_RETRIES = 4


def _parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--assets",
        nargs="*",
        help="Optional whitelist of assets to refresh (default: all configured)",
    )
    parser.add_argument(
        "--public-dir",
        default=os.getenv("PUBLIC_DIR", "public"),
        help="Root directory containing ml_features/, models/ and price caches",
    )
    parser.add_argument(
        "--label-method",
        choices=["realized", "fixed", "tbm"],
        default=DEFAULT_LABEL_METHOD,
        help="Labelling strategy forwarded to make_labels.py",
    )
    parser.add_argument(
        "--label-column",
        default="label",
        help="Name of the label column generated by the labelling step",
    )
    parser.add_argument(
        "--trigger-column",
        default=DEFAULT_TRIGGER_COLUMN,
        help="Name of the trigger column used when deriving TBM events",
    )
    parser.add_argument(
        "--horizon",
        type=int,
        default=DEFAULT_HORIZON,
        help="Forward horizon/vertical barrier when using fixed/tbm methods",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=DEFAULT_THRESHOLD,
        help="Return threshold for the fixed-horizon method",
    )
    parser.add_argument(
        "--pt",
        type=float,
        default=DEFAULT_PT,
        help="Take-profit multiplier for the TBM labelling method",
    )
    parser.add_argument(
        "--sl",
        type=float,
        default=DEFAULT_SL,
        help="Stop-loss multiplier for the TBM labelling method",
    )
    parser.add_argument(
        "--min-samples",
        type=int,
        default=DEFAULT_MIN_SAMPLES,
        help="Minimum labelled rows required before training is attempted",
    )
    parser.add_argument(
        "--skip-label",
        action="store_true",
        help="Reuse an existing labelled dataset instead of regenerating it",
    )
    parser.add_argument(
        "--skip-train",
        action="store_true",
        help="Only run the labelling step without fitting new models",
    )
    parser.add_argument(
        "--skip-readiness",
        action="store_true",
        help="Do not run check_ml_readiness.py after training completes",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Forward --dry-run to train_models.py to skip persisting artefacts",
    )
    return parser.parse_args(argv)


def _normalise_assets(assets: Iterable[str] | None) -> List[str]:
    if not assets:
        return sorted(ASSETS)
    return sorted({asset.upper() for asset in assets})


def _python() -> str:
    return sys.executable or "python3"


def _run_step(label: str, cmd: Sequence[str], env: dict[str, str]) -> None:
    pretty = " ".join(cmd)
    print(f"\n▶ {label}: {pretty}")
    result = subprocess.run(cmd, cwd=_PROJECT_ROOT, env=env)
    if result.returncode != 0:
        raise SystemExit(f"{label} failed with exit code {result.returncode}")


def _summarise_labels(path: Path, label_column: str) -> dict[str, int]:
    summary = {"total": 0, "positive": 0}
    try:
        frame = pd.read_csv(path, usecols=[label_column])
    except FileNotFoundError:
        return summary
    except ValueError:
        try:
            frame = pd.read_csv(path)
        except Exception as exc:  # pragma: no cover - defensive
            print(f"⚠️  Failed to read {path}: {exc}")
            return summary
        if label_column not in frame.columns:
            return summary
    except Exception as exc:  # pragma: no cover - defensive
        print(f"⚠️  Failed to read {path}: {exc}")
        return summary

    series = pd.to_numeric(frame[label_column], errors="coerce").dropna()
    if series.empty:
        return summary

    total = int(series.shape[0])
    positive = int((series > 0).sum())
    summary["total"] = total
    summary["positive"] = positive
    return summary


def _tbm_pt_candidates(initial_pt: float) -> List[float]:
    if initial_pt <= 0:
        return [initial_pt]

    candidates = [initial_pt]
    current = initial_pt
    while len(candidates) < MAX_TBM_PT_RETRIES and current > DEFAULT_TBM_PT_FLOOR:
        current = max(current * 0.5, DEFAULT_TBM_PT_FLOOR)
        if current <= 0:
            break
        if any(math.isclose(current, existing, rel_tol=0.0, abs_tol=1e-6) for existing in candidates):
            break
        candidates.append(current)
        if math.isclose(current, DEFAULT_TBM_PT_FLOOR, rel_tol=0.0, abs_tol=1e-6):
            break
    return candidates


def main(argv: Sequence[str] | None = None) -> int:
    args = _parse_args(argv)
    assets = _normalise_assets(args.assets)

    public_dir = Path(args.public_dir).expanduser().resolve()
    features_dir = public_dir / "ml_features"
    model_dir = public_dir / "models"

    env = os.environ.copy()
    env["PUBLIC_DIR"] = str(public_dir)

    python = _python()

    if not features_dir.exists():
        raise SystemExit(f"Feature directory not found: {features_dir}")

    for asset in assets:
        feature_path = features_dir / f"{asset}_features.csv"
        if not feature_path.exists():
            print(f"⚠️  Skipping {asset}: feature snapshot missing ({feature_path}).")
            continue

        labelled_path = features_dir / f"{asset}_labelled.csv"

        label_stats = {"total": 0, "positive": 0}

        if not args.skip_label:
            pt_candidates: Sequence[float | None]
            if args.label_method == "tbm":
                pt_candidates = _tbm_pt_candidates(args.pt)
            else:
                pt_candidates = [None]

            for attempt_index, pt_candidate in enumerate(pt_candidates):
                label_cmd: List[str] = [
                    python,
                    "scripts/make_labels.py",
                    "--features",
                    str(feature_path),
                    "--method",
                    args.label_method,
                    "--asset",
                    asset,
                    "--price-root",
                    str(public_dir),
                    "--output",
                    str(labelled_path),
                ]
                if args.label_method == "fixed":
                    label_cmd.extend([
                        "--horizon",
                        str(max(args.horizon, 1)),
                        "--threshold",
                        str(args.threshold),
                    ])
                elif args.label_method == "tbm":
                    pt_value = args.pt if pt_candidate is None else pt_candidate
                    label_cmd.extend([
                        "--horizon",
                        str(max(args.horizon, 1)),
                        "--pt",
                        str(pt_value),
                        "--sl",
                        str(args.sl),
                        "--trigger-col",
                        args.trigger_column,
                    ])

                _run_step(f"Label {asset}", label_cmd, env)
                label_stats = _summarise_labels(labelled_path, args.label_column)

                if args.label_method != "tbm" or label_stats["positive"] > 0:
                    break

                if attempt_index < len(pt_candidates) - 1:
                    next_pt = pt_candidates[attempt_index + 1]
                    print(
                        f"⚠️  {asset}: TBM labelling yielded no positive samples with "
                        f"pt={pt_value:g}; retrying with pt={next_pt:g}."
                    )
            else:
                # Exhausted all PT retries without positive samples; stats already captured.
                pass
        else:
            print(f"⚠️  Skipping labelling for {asset} (requested via --skip-label).")
            label_stats = _summarise_labels(labelled_path, args.label_column)

        if args.skip_train:
            print(f"⚠️  Training skipped for {asset} (requested via --skip-train).")
            continue

        sample_count = label_stats["total"]
        if sample_count < max(args.min_samples, 1):
            print(
                "⚠️  Skipping training for {asset}: not enough labelled rows "
                f"({sample_count} < {args.min_samples})."
            )
            continue

        if label_stats["positive"] <= 0:
            print(
                "⚠️  Skipping training for {asset}: no positive samples found in "
                f"{labelled_path}."
            )
            continue
            
        train_cmd: List[str] = [
            python,
            "scripts/train_models.py",
            "--asset",
            asset,
            "--dataset",
            str(labelled_path),
            "--model-dir",
            str(model_dir),
        ]
        if args.dry_run:
            train_cmd.append("--dry-run")
        _run_step(f"Train {asset}", train_cmd, env)

    if not args.skip_readiness and not args.dry_run:
        readiness_cmd = [python, "scripts/check_ml_readiness.py", *assets]
        _run_step("Check ML readiness", readiness_cmd, env)

    return 0


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    raise SystemExit(main())
